# Tokenization
Tokenization is a fundamental step in Natural Language Processing (NLP). It's the process of breaking down text into smaller pieces called tokens. 
<br>
In the context of NLP, a token often refers to a word, but it can also refer to a sentence, paragraph, or any other piece of text, depending on the level of tokenization.
<br>
For example, consider the sentence ```"I love programming."``` If we perform word-level tokenization on this sentence, we get the tokens: "I", "love", "programming".
<br>
Tokenization helps to convert human language into a format that a machine learning model can understand. It's often the first step in preparing text for techniques such as stemming, lemmatization, and vectorization.